/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * The AWS::Rekognition::StreamProcessor type is used to create an Amazon Rekognition StreamProcessor that you can use to analyze streaming videos.
 *
 *
 */
export type AwsRekognitionStreamprocessor = {
  Arn?: Arn;
  /**
   * Name of the stream processor. It's an identifier you assign to the stream processor. You can use it to manage the stream processor.
   */
  Name?: string;
  /**
   * The KMS key that is used by Rekognition to encrypt any intermediate customer metadata and store in the customer's S3 bucket.
   */
  KmsKeyId?: string;
  /**
   * ARN of the IAM role that allows access to the stream processor, and provides Rekognition read permissions for KVS stream and write permissions to S3 bucket and SNS topic.
   */
  RoleArn: string;
  KinesisVideoStream: KinesisVideoStream;
  FaceSearchSettings?: FaceSearchSettings;
  ConnectedHomeSettings?: ConnectedHomeSettings;
  KinesisDataStream?: KinesisDataStream;
  S3Destination?: S3Destination;
  NotificationChannel?: NotificationChannel;
  DataSharingPreference?: DataSharingPreference;
  /**
   * The PolygonRegionsOfInterest specifies a set of polygon areas of interest in the video frames to analyze, as part of connected home feature. Each polygon is in turn, an ordered list of Point
   *
   * @minItems 0
   */
  PolygonRegionsOfInterest?: Polygon[];
  /**
   * The BoundingBoxRegionsOfInterest specifies an array of bounding boxes of interest in the video frames to analyze, as part of connected home feature. If an object is partially in a region of interest, Rekognition will tag it as detected if the overlap of the object with the region-of-interest is greater than 20%.
   *
   * @minItems 0
   */
  BoundingBoxRegionsOfInterest?: BoundingBox[];
  /**
   * Current status of the stream processor.
   */
  Status?: string;
  /**
   * Detailed status message about the stream processor.
   */
  StatusMessage?: string;
  /**
   * An array of key-value pairs to apply to this resource.
   *
   * @minItems 0
   * @maxItems 200
   */
  Tags?: Tag[];
} & {
  [k: string]: unknown;
};
/**
 * The ARN of the stream processor
 */
export type Arn = string;
/**
 * List of labels that need to be detected in the video stream. Current supported values are PERSON, PET, PACKAGE, ALL.
 *
 * @minItems 1
 */
export type Labels = [string, ...string[]];
/**
 * A polygon showing a region of interest. Note that the ordering of the Point entries matter in defining the polygon
 *
 * @minItems 3
 */
export type Polygon = [Point, Point, Point, ...Point[]];

/**
 * The Kinesis Video Stream that streams the source video.
 */
export interface KinesisVideoStream {
  /**
   * ARN of the Kinesis Video Stream that streams the source video.
   */
  Arn: string;
}
/**
 * Face search settings to use on a streaming video. Note that either FaceSearchSettings or ConnectedHomeSettings should be set. Not both
 */
export interface FaceSearchSettings {
  /**
   * The ID of a collection that contains faces that you want to search for.
   */
  CollectionId: string;
  /**
   * Minimum face match confidence score percentage that must be met to return a result for a recognized face. The default is 80. 0 is the lowest confidence. 100 is the highest confidence. Values between 0 and 100 are accepted.
   */
  FaceMatchThreshold?: number;
}
/**
 * Connected home settings to use on a streaming video. Note that either ConnectedHomeSettings or FaceSearchSettings should be set. Not both
 */
export interface ConnectedHomeSettings {
  Labels: Labels;
  /**
   * Minimum object class match confidence score that must be met to return a result for a recognized object.
   */
  MinConfidence?: number;
}
/**
 * The Amazon Kinesis Data Stream stream to which the Amazon Rekognition stream processor streams the analysis results, as part of face search feature.
 */
export interface KinesisDataStream {
  /**
   * ARN of the Kinesis Data Stream stream.
   */
  Arn: string;
}
/**
 * The S3 location in customer's account where inference output & artifacts are stored, as part of connected home feature.
 */
export interface S3Destination {
  /**
   * Name of the S3 bucket.
   */
  BucketName: string;
  /**
   * The object key prefix path where the results will be stored. Default is no prefix path
   */
  ObjectKeyPrefix?: string;
}
/**
 * The ARN of the SNS notification channel where events of interests are published, as part of connected home feature.
 */
export interface NotificationChannel {
  /**
   * ARN of the SNS topic.
   */
  Arn: string;
}
/**
 * Indicates whether Rekognition is allowed to store the video stream data for model-training.
 */
export interface DataSharingPreference {
  /**
   * Flag to enable data-sharing
   */
  OptIn: boolean;
}
/**
 * An (X, Y) cartesian coordinate denoting a point on the frame
 */
export interface Point {
  /**
   * The X coordinate of the point.
   */
  X: number;
  /**
   * The Y coordinate of the point.
   */
  Y: number;
}
/**
 * A bounding box denoting a region of interest in the frame to be analyzed.
 */
export interface BoundingBox {
  Height: number;
  Width: number;
  Left: number;
  Top: number;
}
/**
 * A key-value pair to associate with a resource.
 */
export interface Tag {
  /**
   * The key name of the tag. You can specify a value that is 1 to 128 Unicode characters in length and cannot be prefixed with aws:. You can use any of the following characters: the set of Unicode letters, digits, whitespace, _, ., /, =, +, and -.
   */
  Key: string;
  /**
   * The value for the tag. You can specify a value that is 0 to 256 Unicode characters in length and cannot be prefixed with aws:. You can use any of the following characters: the set of Unicode letters, digits, whitespace, _, ., /, =, +, and -.
   */
  Value: string;
}
